{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaRED Library Quick Start DEMO\n",
    "\n",
    "This demonstration is a step by step on how to use the SpaRED library to train a gene expression prediction model with pre-trained weights. In this tutorial we will illustrate how to:\n",
    "\n",
    "* Load a SpaRED dataset and save it in an adata \n",
    "* Prepare the data for training a pretrained model for gene expression prediction\n",
    "* Initialize a pretrained model\n",
    "* Train the model using pytorch lighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "currentdir = os.getcwd()\n",
    "parentdir = str(Path(currentdir).parents[2])\n",
    "sys.path.insert(0, parentdir)\n",
    "\n",
    "import spared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets\n",
    "\n",
    "The `datasets` file has a function to get any desired dataset and return the adata as well as the parameter dictionary. This function returns a filtered and processed adata. This function has a parameter called *visualize* that allows for all visualizations if set to True. The fuction also saves the raw_adata (not processed) in case it is required. \n",
    "\n",
    "We will begin by loading a dataset and setting the *visualize* parameter as False since no images are required for the functions analized in this DEMO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvegaa/anaconda3/envs/H2ST/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vicari_mouse_brain dataset with the following data split:\n",
      "train data: ['V11L12-038_A1', 'V11L12-038_B1', 'V11L12-038_C1', 'V11L12-038_D1', 'V11L12-109_A1', 'V11L12-109_B1', 'V11L12-109_C1', 'V11L12-109_D1']\n",
      "val data: ['V11T16-085_A1', 'V11T16-085_B1', 'V11T16-085_C1', 'V11T16-085_D1']\n",
      "test data: ['V11T17-101_A1', 'V11T17-101_B1']\n",
      "Parameters already saved in /media/SSD4/dvegaa/SpaRED/spared/processed_data/vicari_data/vicari_mouse_brain/2024-07-08-11-11-47/parameters.json\n",
      "Loading main adata file from disk (/media/SSD4/dvegaa/SpaRED/docs/notebooks/tutorials/processed_data/vicari_data/vicari_mouse_brain/2024-07-08-11-11-47/adata.h5ad)...\n",
      "The loaded adata object looks like this:\n",
      "AnnData object with n_obs × n_vars = 43804 × 128\n",
      "    obs: 'in_tissue', 'array_row', 'array_col', 'patient', 'slide_id', 'split', 'unique_id', 'n_genes_by_counts', 'total_counts'\n",
      "    var: 'gene_ids', 'feature_types', 'genome', 'gene_symbol', 'exp_frac', 'glob_exp_frac', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'gene_length', 'd_log1p_moran', 'log1p_avg_exp', 'd_log1p_avg_exp', 'c_log1p_avg_exp', 'c_d_log1p_avg_exp'\n",
      "    uns: 'spatial'\n",
      "    obsm: 'patches_scale_1.0', 'spatial'\n",
      "    layers: 'c_d_deltas', 'c_d_log1p', 'c_deltas', 'c_log1p', 'counts', 'd_deltas', 'd_log1p', 'deltas', 'log1p', 'mask', 'tpm'\n"
     ]
    }
   ],
   "source": [
    "from spared.datasets import get_dataset\n",
    "\n",
    "#get dataset\n",
    "data = get_dataset(\"vicari_mouse_brain\", visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "To train a pretrained model for gene expression prediction we must first prepare the data. We use Dataloaders, a component commonly used in machine learning frameworks like PyTorch to handle the loading of data in an efficient and flexible manner. The function `get_pretraine_dataloaders` receives the following parameters as input:\n",
    "\n",
    "* **adata (ad.AnnData):** AnnData object to process\n",
    "* **layer (str):** the layer to use for pre-training\n",
    "* **batch_size (int):** the batch size of the loaders. Default is set to 128\n",
    "* **shuffle (bool):** whether to shuffle the data in the loaders\n",
    "* **cuda (bool):** whether to use cuda in the loaders\n",
    "\n",
    "And returns train, valid and test dataloaders as a Tuple to training a pretrained model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using noisy_delta layer for training. This will probably yield bad results.\n",
      "Percentage of imputed observations with median filter: 27.503%\n"
     ]
    }
   ],
   "source": [
    "from spared.dataloaders import get_pretrain_dataloaders\n",
    "\n",
    "# Declare train and test loaders\n",
    "train_dataloader, val_dataloader, test_dataloader = get_pretrain_dataloaders(\n",
    "    adata=data.adata,\n",
    "    layer = 'c_d_log1p',\n",
    "    batch_size = 265,\n",
    "    shuffle = True,\n",
    "    use_cuda = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model\n",
    "\n",
    "The SpaRED library provides a function to  initilize a model with pretrained weights for gene expression prediction. The function `ImageEncoder` receives as input:\n",
    "\n",
    "* **args (argparse):** argparse with specifid variables required to initialize the model\n",
    "* **latem_dim (int):** latent dimensions used as output feature dimensions in the last layer of the defined model.\n",
    "\n",
    "And returns the model. The backbone used as pretrained weights is defined in the argparse parameter. We will use *ShuffleNetV2* in this DEMO, however, you can check the available backbones in the documentation of our SpaRED library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageBackbone(\n",
       "  (test_transforms): Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "  (criterion): MSELoss()\n",
       "  (encoder): ShuffleNetV2(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (stage2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (branch1): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "          (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (branch1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (branch1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv5): Sequential(\n",
       "      (0): Conv2d(192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spared.models import ImageBackbone\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "# Define argparse variables\n",
    "test_args = argparse.Namespace()\n",
    "arg_dict = vars(test_args)\n",
    "input_dict = {\n",
    "    'img_backbone': 'ShuffleNetV2',\n",
    "    'img_use_pretrained': True,\n",
    "    'average_test': False,\n",
    "    'optim_metric': 'MSE',\n",
    "    'robust_loss': False,\n",
    "    'optimizer': 'Adam',\n",
    "    'lr': 0.0001,\n",
    "    'momentum': 0.9,\n",
    "}\n",
    "\n",
    "for key,value in input_dict.items():\n",
    "    arg_dict[key]= value\n",
    "\n",
    "\n",
    "# Declare device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageBackbone(args=test_args,  latent_dim=data.adata.n_vars).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Now we will use pytorch lighting to train the model. First we must define a `ModelCheckpoint` callback to monitor the validation mean squared error (val_MSE) and save only the best model based on this metric. Then we initialize a `Trainer` and define various parameters. In this case our model will run for a maximum of 1000 training steps, perform validation every 10 steps, log progress every 10 steps, and use one GPU for training. Additionally, the trainer displays a progress bar and a model summary during training. This setup ensures efficient model training and validation, with automatic saving of the best model based on the specified validation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define checkpoint callback to save best model in validation\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=f'val_MSE', # Choose your validation metric\n",
    "    save_top_k=1, # Save only the best model\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Define the trainier and fit the model\n",
    "trainer = Trainer(\n",
    "    max_steps=100,\n",
    "    val_check_interval=10,\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    check_val_every_n_epoch=None,\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must begin the training process using PyTorch Lightning's `Trainer` with the `fit` method. The `fit` method is called with the specified model, training data loader (train_dataloader), and validation data loader (val_dataloader). This setup allows the Trainer to manage the training loop, including feeding the model with training data, performing validation at specified intervals, and utilizing the previously defined configurations and callbacks for efficient training and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /media/SSD4/dvegaa/SpaRED/docs/notebooks/tutorials/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name            | Type         | Params\n",
      "-------------------------------------------------\n",
      "0 | test_transforms | Normalize    | 0     \n",
      "1 | criterion       | MSELoss      | 0     \n",
      "2 | encoder         | ShuffleNetV2 | 472 K \n",
      "-------------------------------------------------\n",
      "472 K     Trainable params\n",
      "0         Non-trainable params\n",
      "472 K     Total params\n",
      "1.892     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  11%|█         | 10/90 [00:51<06:53,  0.19it/s, v_num=0]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  11%|█         | 10/90 [00:51<06:53,  0.19it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models performance\n",
    "\n",
    "After training, the path to the best model is obtained from the `checkpoint_callback`. We can use the best model to evaluate performance on the test set (if available). We load the model from the `checkpoint_callback` path and use the `trainer.test` method to evaluate the model's performance on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 10.49it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       test_Global          -3.1575732231140137\n",
      "        test_MAE            1.0347423553466797\n",
      "        test_MSE            1.7111455202102661\n",
      "      test_PCC-Gene         0.12955906987190247\n",
      "     test_PCC-Patch          0.879209578037262\n",
      "      test_R2-Gene          -1.7244747877120972\n",
      "      test_R2-Patch         0.3040209114551544\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# Load the best model after training\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model = ImageBackbone.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Test model if there is a test dataloader\n",
    "if not (test_dataloader is None):\n",
    "    trainer.test(model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
