{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaRED Library MISC DEMO\n",
    "\n",
    "This demonstration illustrates the usage of various functions available in our SpaRED PyPI library. The functions we will explore are categorized as follows:\n",
    "\n",
    "* Spot Features\n",
    "* Graph operations\n",
    "* Dataloaders\n",
    "* Models\n",
    "* Metrics\n",
    "\n",
    "These functions provide essential tools for preparing your data for model training and inference, as well as for evaluating model performance. By leveraging these capabilities, users can streamline their workflow and enhance the efficiency and accuracy of their spatial transcriptomics analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/SSD4/dvegaa/SpaRED\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as im\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "currentdir = os.getcwd()\n",
    "parentdir = str(Path(currentdir).parents[2])\n",
    "sys.path.insert(0, parentdir)\n",
    "print(parentdir)\n",
    "\n",
    "import spared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets\n",
    "\n",
    "The `datasets` file has a function to get any desired dataset and return the adata as well as the parameter dictionary. This function returns a filtered and processed adata. This function has a parameter called *visualize* that allows for all visualizations if set to True. The fuction also saves the raw_adata (not processed) in case it is required. \n",
    "\n",
    "We will begin by loading a dataset and setting the *visualize* parameter as False since no images are required for the functions analized in this DEMO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vicari_mouse_brain dataset with the following data split:\n",
      "train data: ['V11L12-038_A1', 'V11L12-038_B1', 'V11L12-038_C1', 'V11L12-038_D1', 'V11L12-109_A1', 'V11L12-109_B1', 'V11L12-109_C1', 'V11L12-109_D1']\n",
      "val data: ['V11T16-085_A1', 'V11T16-085_B1', 'V11T16-085_C1', 'V11T16-085_D1']\n",
      "test data: ['V11T17-101_A1', 'V11T17-101_B1']\n",
      "Parameters already saved in /media/SSD4/dvegaa/SpaRED/spared/processed_data/vicari_data/vicari_mouse_brain/2024-07-08-11-11-47/parameters.json\n",
      "Loading main adata file from disk (/media/SSD4/dvegaa/SpaRED/docs/notebooks/tutorials/processed_data/vicari_data/vicari_mouse_brain/2024-07-08-11-11-47/adata.h5ad)...\n",
      "The loaded adata object looks like this:\n",
      "AnnData object with n_obs × n_vars = 43804 × 128\n",
      "    obs: 'in_tissue', 'array_row', 'array_col', 'patient', 'slide_id', 'split', 'unique_id', 'n_genes_by_counts', 'total_counts'\n",
      "    var: 'gene_ids', 'feature_types', 'genome', 'gene_symbol', 'exp_frac', 'glob_exp_frac', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'gene_length', 'd_log1p_moran', 'log1p_avg_exp', 'd_log1p_avg_exp', 'c_log1p_avg_exp', 'c_d_log1p_avg_exp'\n",
      "    uns: 'spatial'\n",
      "    obsm: 'patches_scale_1.0', 'spatial'\n",
      "    layers: 'c_d_deltas', 'c_d_log1p', 'c_deltas', 'c_log1p', 'counts', 'd_deltas', 'd_log1p', 'deltas', 'log1p', 'mask', 'tpm'\n"
     ]
    }
   ],
   "source": [
    "from spared.datasets import get_dataset\n",
    "import anndata as ad\n",
    "\n",
    "#get dataset\n",
    "data = get_dataset(\"vicari_mouse_brain\", visualize=False)\n",
    "\n",
    "#adata\n",
    "adata = data.adata\n",
    "\n",
    "#parameters dictionary\n",
    "param_dict = data.param_dict\n",
    "\n",
    "#loading raw adata \n",
    "dataset_path = os.getcwd()\n",
    "files_path = os.path.join(dataset_path, \"processed_data/vicari_data/vicari_mouse_brain/\")\n",
    "files = os.listdir(files_path)\n",
    "adata_path = os.path.join(files_path, files[0], \"adata_raw.h5ad\")\n",
    "raw_adata = ad.read_h5ad(adata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to explore the functions one by one. This tutorial will demostrate how to use each function, what to introduce as input and the expected output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot Features Functions\n",
    "\n",
    "We will begin with the spot feature functions available in the SpaRED library. These functions are crucial for extracting and analyzing the unique characteristics and spatial relationships of each spot within spatial transcriptomics data. \n",
    "\n",
    "Lets begin with `compute_patches_embeddings`. This function receives as input:\n",
    "\n",
    "* **adata (ad.AnnData):** AnnData object to process\n",
    "* **backbone (str):** backbone model to use\n",
    "* **model_path (str):** path to where the model will be saved\n",
    "* **patch_size (int):** size of the patches\n",
    "\n",
    "And computes embeddings for a given backbone model and adata object. The embeddings are stored in adata.obsm[f'embeddings_{backbone}'] in the processed adata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 172/172 [01:00<00:00,  2.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 43804 × 128\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'patient', 'slide_id', 'split', 'unique_id', 'n_genes_by_counts', 'total_counts'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'gene_symbol', 'exp_frac', 'glob_exp_frac', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'gene_length', 'd_log1p_moran', 'log1p_avg_exp', 'd_log1p_avg_exp', 'c_log1p_avg_exp', 'c_d_log1p_avg_exp'\n",
       "    uns: 'spatial'\n",
       "    obsm: 'patches_scale_1.0', 'spatial', 'embeddings_densenet'\n",
       "    layers: 'c_d_deltas', 'c_d_log1p', 'c_deltas', 'c_log1p', 'counts', 'd_deltas', 'd_log1p', 'deltas', 'log1p', 'mask', 'tpm'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spared.spot_features import compute_patches_embeddings\n",
    "\n",
    "compute_patches_embeddings(adata=adata, backbone='densenet', model_path=\"None\", patch_size= 224)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_patches_predictions` receives as input:\n",
    "\n",
    "* **adata (ad.AnnData):** AnnData object to process\n",
    "* **backbone (str):** backbone model to use\n",
    "* **model_path (str):** path to where the model will be saved\n",
    "* **patch_size (int):** size of the patches\n",
    "\n",
    "And computes predictions for a given backbone model and adata object. The predictions are stored in adata.obsm[f'predictions_{backbone}'] in the processed adata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting predictions: 100%|██████████| 172/172 [00:59<00:00,  2.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 43804 × 128\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'patient', 'slide_id', 'split', 'unique_id', 'n_genes_by_counts', 'total_counts'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'gene_symbol', 'exp_frac', 'glob_exp_frac', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'gene_length', 'd_log1p_moran', 'log1p_avg_exp', 'd_log1p_avg_exp', 'c_log1p_avg_exp', 'c_d_log1p_avg_exp'\n",
       "    uns: 'spatial'\n",
       "    obsm: 'patches_scale_1.0', 'spatial', 'embeddings_densenet', 'predictions_densenet'\n",
       "    layers: 'c_d_deltas', 'c_d_log1p', 'c_deltas', 'c_log1p', 'counts', 'd_deltas', 'd_log1p', 'deltas', 'log1p', 'mask', 'tpm'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spared.spot_features import compute_patches_predictions\n",
    "\n",
    "compute_patches_predictions(adata=adata, backbone='densenet', model_path=\"None\", patch_size= 224)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_dim_red` receives as input:\n",
    "\n",
    "* **adata (ad.AnnData):** AnnData object to process\n",
    "* **from_layer (str):** the key in adata.layers where the expression matrix is stored\n",
    "\n",
    "And returns the adata with computed embeddings and clusters. The clusters are computed using Leiden algorithm and are stored in adata.obs[\"cluster\"]. This function also reduce the dimensionality of the adata by a method called Principal Component Analysis. The results are stored in adata.obsm['X_pca']. Adittionaly, this function also performs the UMAP (Uniform Manifold Approximation and Projection), which helps you visualize your high-dimensional data in 2D or 3D. The results are stored in adata.obsm['X_umap']. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 43804 × 128\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'patient', 'slide_id', 'split', 'unique_id', 'n_genes_by_counts', 'total_counts', 'cluster'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'gene_symbol', 'exp_frac', 'glob_exp_frac', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'gene_length', 'd_log1p_moran', 'log1p_avg_exp', 'd_log1p_avg_exp', 'c_log1p_avg_exp', 'c_d_log1p_avg_exp'\n",
       "    uns: 'spatial', 'pca', 'neighbors', 'umap', 'leiden'\n",
       "    obsm: 'patches_scale_1.0', 'spatial', 'embeddings_densenet', 'predictions_densenet', 'X_pca', 'X_umap'\n",
       "    varm: 'PCs'\n",
       "    layers: 'c_d_deltas', 'c_d_log1p', 'c_deltas', 'c_log1p', 'counts', 'd_deltas', 'd_log1p', 'deltas', 'log1p', 'mask', 'tpm'\n",
       "    obsp: 'distances', 'connectivities'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spared.spot_features import compute_dim_red\n",
    "\n",
    "adata = compute_dim_red(adata=adata, from_layer=\"c_d_log1p\")\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_spatial_neighbors` receives as input:\n",
    "\n",
    "* **adata (ad.AnnData):** AnnData object to process\n",
    "* **n_hops (int):** the size of the neighborhood to take into account to compute the neighbors where the expression matrix is stored\n",
    "* **hex_geometry (bool):** whether the graph is hexagonal (True) or a grid (False)\n",
    "\n",
    "And returns a neighbors dictionary for an AnnData object where the keys are the ids of each spot and the values correspond to a list of neighbor spots. The neighbors are computed according to topological distances over a graph defined by the hex_geometry connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spared.spot_features import get_spatial_neighbors\n",
    "\n",
    "dict_spatial_neighbors = get_spatial_neighbors(adata=adata, n_hops=6, hex_geometry=param_dict[\"hex_geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Operations Functions\n",
    "\n",
    "In this section, we will explore the graph operations functions available in the SpaRED library. These functions are essential for converting spatial transcriptomics data into geometric graphs, where the central nodes represent spots and the adjacent nodes represent the spot's neighbors. These graphs integrate information from multiple patches, enabling the prediction of gene expression for the central node by leveraging the spatial information from its neighbors.\n",
    "\n",
    "Lets begin with `get_graphs_one_slide`. This function receives as input:\n",
    "\n",
    "* **adata (ad.AnnData):** Slide AnnData object to process\n",
    "* **n_hops (int):** the number of hops to compute the graph\n",
    "* **layer:** the layer of the graph to predict, which will be added as *y* to the graph\n",
    "* **hex_geometry (bool):** whether the graph has a hexagonal geometry or not\n",
    "\n",
    "And returns a Tuple. The first position corresponds to a dictionary where the patch names are the keys and pytorch geometric graph for each one as values. The second position is an integer corresponding to the maximun column or row difference between the center and the neighbors.\n",
    "\n",
    "Before using this function, the patch embeddings and predictions must be computed using the following functions:\n",
    "* `compute_patches_embeddings`\n",
    "* `compute_patches_predictions`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 172/172 [01:00<00:00,  2.86it/s]\n",
      "Getting predictions: 100%|██████████| 172/172 [01:01<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from spared.graph_operations import get_graphs_one_slide\n",
    "\n",
    "#Graph operation must have embedding and prediction layers\n",
    "compute_patches_embeddings(adata=adata, backbone='densenet', model_path=\"None\", patch_size= 224)\n",
    "compute_patches_predictions(adata=adata, backbone='densenet', model_path=\"None\", patch_size= 224)\n",
    "\n",
    "#Get slide adata\n",
    "slide_id = adata.obs.slide_id.unique()[0]\n",
    "slide_adata = adata[adata.obs.slide_id == slide_id]\n",
    "\n",
    "#Get graph for one slide\n",
    "dict_graph_slide, max_pos = get_graphs_one_slide(adata=slide_adata, n_hops=6, layer=\"c_d_log1p\", hex_geometry=param_dict[\"hex_geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`get_sin_cos_positional_embeddings` receives as input:\n",
    "\n",
    "* **graph_dict (dict):** dictionary where the patch names are the keys and a pytorch geometric graphs for each one are values\n",
    "* **max_d_pos (int):** the maximun absolute value in the relative position matrix\n",
    "\n",
    "And adds a transformer-like positional encodings to each graph in a graph dict. It adds the positional\n",
    "encodings under the attribute 'positional_embeddings' for each graph. \n",
    "\n",
    "The input for this function correspond to the output of `get_graphs_one_slide`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spared.graph_operations import get_sin_cos_positional_embeddings\n",
    "\n",
    "dict_pos_emb = get_sin_cos_positional_embeddings(graph_dict=dict_graph_slide, max_d_pos=max_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_graphs` receives as input:\n",
    "\n",
    "* **adata (ad.AnnData):** AnnData object to build the graphs\n",
    "* **n_hops (int):** the number of hops to compute each graph\n",
    "* **layer (str):** the layer of the graph to predict, which will be added as *y* to the graph\n",
    "* **hex_geometry (bool):** whether the graph has a hexagonal geometry or not\n",
    "\n",
    "And returns a dictionary where the spot names are the keys and pytorch geometric graphs are the values. \n",
    "\n",
    "Before using this function, the patch embeddings and predictions must be computed using the following functions:\n",
    "* `compute_patches_embeddings`\n",
    "* `compute_patches_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 172/172 [01:00<00:00,  2.85it/s]\n",
      "Getting predictions: 100%|██████████| 172/172 [01:01<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:24<00:00,  6.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from spared.graph_operations import get_graphs\n",
    "\n",
    "#Graph operation must have embedding and prediction layers\n",
    "compute_patches_embeddings(adata=adata, backbone='densenet', model_path=\"None\", patch_size= 224)\n",
    "compute_patches_predictions(adata=adata, backbone='densenet', model_path=\"None\", patch_size= 224)\n",
    "\n",
    "#Get graphs\n",
    "dict_graphs = get_graphs(adata=adata, n_hops=6, layer=\"c_d_log1p\", hex_geometry=param_dict[\"hex_geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders Functions\n",
    "\n",
    "`get_pretrain_dataloaders` receives as input:\n",
    "\n",
    "* **adata (ad.AnnData):** AnnData object to process\n",
    "* **layer (str):** the layer used for pre-training. Default set to *deltas*\n",
    "* **batch_size (int):** the batch size of the loader. Default set to 128\n",
    "* **shuffle (bool):** whether to shuffle the data in the loaders\n",
    "* **use_cude (bool):** whether to use cuda or not\n",
    "\n",
    "And returns a Tuple containing the train, validation and test dataloaders in the form of *AnnLoader*. If there is no test, the test dataloader in *None*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using noisy_delta layer for training. This will probably yield bad results.\n",
      "Percentage of imputed observations with median filter: 27.503%\n"
     ]
    }
   ],
   "source": [
    "from spared.dataloaders import get_pretrain_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader = get_pretrain_dataloaders(adata = adata, layer = 'c_d_log1p', batch_size = 128, shuffle = True, use_cuda = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_pretrain_dataloaders` receives as input:\n",
    "\n",
    "* **adata (ad.AnnData):** AnnData object to process\n",
    "* **dataset_path (str):** the path to the dataset (where the graphs will be stored). Defaults set to ''\n",
    "* **layer (str):** the layer used to predict. Default set to 'c_t_log1p'\n",
    "* **n_hops (int):** number of hops to compute the graph. Defaults set to 2\n",
    "* **backbone (str):** backbone model to use. Default set to *densenet*\n",
    "* **model_path (str):** path to the model to use. Default set to *None*\n",
    "* **batch_size (int):** batch size of the dataloaders. Default set to 128\n",
    "* **shuffle (bool):** whether to shuffle the fata in the dataloaders. Default set to *True*\n",
    "* **hex_geometry (bool):** whether the graph is hexagonal or not. Defaults set to *True*\n",
    "* **patch_size (int):** size of the patches.Default set to 224\n",
    "\n",
    "And returns a Tuple containing the train, validation and test graphs dataloaders in the form of *geo_DataLoader*. This function performs all the pipeline to get graphs dataloaders for a dataset, which includes:\n",
    "\n",
    "1. Computes embeddings and predictions for the patches using the specified backbone and model.\n",
    "2. Computes the graph dictionaries for the dataset using the embeddings and predictions.\n",
    "3. Saves the graphs in the dataset_path folder.\n",
    "4. Returns the train, validation and test dataloaders for the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs not found in file, computing graphs...\n",
      "Using noisy_delta layer for training. This will probably yield bad results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting embeddings: 100%|██████████| 172/172 [01:05<00:00,  2.63it/s]\n",
      "Getting predictions: 100%|██████████| 172/172 [01:05<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:47<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving graphs...\n"
     ]
    }
   ],
   "source": [
    "from spared.dataloaders import get_graph_dataloaders\n",
    "\n",
    "#Path to where the graphs will be saved\n",
    "graphs_path = os.path.join(parentdir, \"processed_data/vicari_data/vicari_mouse_brain/graphs\")\n",
    "os.makedirs(graphs_path, exist_ok=True)\n",
    "\n",
    "train_graph_loader, val_graph_loader, test_graph_loader = get_graph_dataloaders(adata = adata, dataset_path = graphs_path, layer = 'c_d_log1p', n_hops = 2, backbone = 'densenet', model_path = \"None\", batch_size = 128, shuffle = True, hex_geometry = param_dict[\"hex_geometry\"], patch_size = 224)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Functions\n",
    "\n",
    "`ImageEnconder` receives as input:\n",
    "\n",
    "* **backbone (str):** name of the models' backbone\n",
    "* **use_pretrained (bool):** whether to use pretrained weights (True) or not (False)\n",
    "* **latent_dim (int):** the dimension of the latent representation \n",
    "\n",
    "And returns the Image Encoder model with the specified backbone. The backbones available are display in the SpaRED library documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoder(\n",
       "  (encoder): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spared.models import ImageEncoder\n",
    "\n",
    "model = ImageEncoder(backbone='resnet', use_pretrained=True, latent_dim=adata.n_vars)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ImageBackbone` receives as input:\n",
    "\n",
    "* **args (argparse):** argparse with specifid variables required to initialize the model\n",
    "* **latem_dim (int):** latent dimensions used as output feature dimensions in the last layer of the defined model.\n",
    "\n",
    "And returns the model with the specified backbone. The example provided shows the variables that should be defined in the *argparse*. The backbone that we will be using for the pretrained weights is *ShuffleNetV2*, however, you can check the available backbones in the SpaRED library documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(average_test=False, img_backbone='ShuffleNetV2', img_use_pretrained=True, lr=0.0001, momentum=0.9, optim_metric='MSE', optimizer='Adam', robust_loss=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageBackbone(\n",
       "  (test_transforms): Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "  (criterion): MSELoss()\n",
       "  (encoder): ShuffleNetV2(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (stage2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (branch1): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "          (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (branch1): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (branch1): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (branch1): Sequential()\n",
       "        (branch2): Sequential(\n",
       "          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (7): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv5): Sequential(\n",
       "      (0): Conv2d(192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spared.models import ImageBackbone\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "# Define argparse variables\n",
    "args = argparse.Namespace()\n",
    "arg_dict = vars(args)\n",
    "\n",
    "input_dict = {\n",
    "    'img_backbone': 'ShuffleNetV2',\n",
    "    'img_use_pretrained': True,\n",
    "    'average_test': False,\n",
    "    'optim_metric': 'MSE',\n",
    "    'robust_loss': False,\n",
    "    'optimizer': 'Adam',\n",
    "    'lr': 0.0001,\n",
    "    'momentum': 0.9,\n",
    "}\n",
    "\n",
    "for key,value in input_dict.items():\n",
    "    arg_dict[key]= value\n",
    "\n",
    "print(args)\n",
    "\n",
    "# Declare device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageBackbone(args=args,  latent_dim=data.adata.n_vars).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Functions\n",
    "\n",
    "Now lets delve into the metrics functions available in the SpaRED library. These functions are crucial for evaluating the performance of your models. By using the provided metrics, you can accurately assess various aspects of model accuracy, such as prediction errors and the quality of spatial predictions. These metrics help in quantifying the effectiveness of your models in predicting gene expression.\n",
    "\n",
    "Lets begin with `get_pearsonr`. This function receives as input:\n",
    "\n",
    "* **gt_mat (torch.Tensor):** ground truth matrix of shape (n_observations, n_variables)\n",
    "* **pred_mat (torch.Tensor):** predicted matrix of shape (n_observations, n_variables).\n",
    "* **mask (torch.Tensor):** boolean mask with False in positions that must be ignored in metric computation (n_observations, n_variables).\n",
    "* **axis (int):** wether to compute the pcc by columns (axis=0) ir by rows (axis=1)\n",
    "\n",
    "And returns a Tuple. The first position corresponds to the Mean Pearson correlation computed by averaging the Pearson correlation for each patch. The second position corresponds to a list of the Pearson Correlation Coeficient for each one of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be generating random prediction and ground truth matrices as well as a random mask where *26%* of the values will be masked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set number of observations and genes (hypothetical)\n",
    "obs = 10\n",
    "genes = 8\n",
    "imputed_fraction = 0.26 # This is the percentage of zeros in the mask\n",
    "\n",
    "# Henerate random matrices\n",
    "pred = torch.randn((obs,genes))\n",
    "gt = torch.randn((obs,genes))\n",
    "mask = torch.rand((obs,genes))>imputed_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC by columns: -0.18081757426261902\n",
      "[-0.5434356331825256, 0.2459755688905716, 0.7937927842140198, -0.3614409863948822, -0.3860357403755188, -0.06298591196537018, -0.6647549271583557, -0.4676556885242462]\n",
      "PCC by rows: -0.06456981599330902\n",
      "[-0.10244599729776382, -0.30918097496032715, 0.8755841255187988, 0.16263891756534576, -0.5979809165000916, 0.22747549414634705, -0.2783399522304535, -0.32421642541885376, 0.02806819975376129, -0.32730066776275635]\n"
     ]
    }
   ],
   "source": [
    "from spared.metrics import get_pearsonr\n",
    "\n",
    "mean_pcc_col, list_pcc_col = get_pearsonr(gt_mat=gt, pred_mat=pred, mask=mask, axis=0)\n",
    "mean_pcc_row, list_pcc_row = get_pearsonr(gt_mat=gt, pred_mat=pred, mask=mask, axis=1)\n",
    "\n",
    "print(\"PCC by columns: \" + str(mean_pcc_col))\n",
    "print(list_pcc_col)\n",
    "\n",
    "print(\"PCC by rows: \" + str(mean_pcc_row))\n",
    "print(list_pcc_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_r2_score` receives as input:\n",
    "\n",
    "* **gt_mat (torch.Tensor):** ground truth matrix of shape (n_observations, n_variables)\n",
    "* **pred_mat (torch.Tensor):** predicted matrix of shape (n_observations, n_variables).\n",
    "* **mask (torch.Tensor):** boolean mask with False in positions that must be ignored in metric computation (n_observations, n_variables).\n",
    "* **axis (int):** wether to compute the pcc by columns (axis=0) ir by rows (axis=1)\n",
    "\n",
    "And returns a Tuple. The first position corresponds to the Mean R2 score computed by averaging the R2 score for each column in the matrices. The second position corresponds to a list of R2 scores for each one of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be generating random prediction and ground truth matrices as well as a random mask where *26%* of the values will be masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set number of observations and genes (hypothetical)\n",
    "obs = 10\n",
    "genes = 8\n",
    "imputed_fraction = 0.26 # This is the percentage of zeros in the mask\n",
    "\n",
    "# Henerate random matrices\n",
    "pred = torch.randn((obs,genes))\n",
    "gt = torch.randn((obs,genes))\n",
    "mask = torch.rand((obs,genes))>imputed_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score by columns: -1.4981153011322021\n",
      "[-0.34498167037963867, -2.279686689376831, -2.8530077934265137, -1.471015214920044, -0.3273383378982544, -1.969834327697754, -0.3219904899597168, -2.4170680046081543]\n",
      "R2 Score by rows: -1.3743867874145508\n",
      "[-0.8520510196685791, -0.5073205232620239, -1.1874570846557617, -0.6010171175003052, -2.011044979095459, -0.8247621059417725, -1.4701313972473145, -1.2510161399841309, -4.709837913513184, -0.3292292356491089]\n"
     ]
    }
   ],
   "source": [
    "from spared.metrics import get_r2_score\n",
    "\n",
    "mean_r2_col, list_r2_col = get_r2_score(gt_mat=gt, pred_mat=pred, mask=mask, axis=0)\n",
    "mean_r2_row, list_r2_row = get_r2_score(gt_mat=gt, pred_mat=pred, mask=mask, axis=1)\n",
    "\n",
    "print(\"R2 Score by columns: \" + str(mean_r2_col))\n",
    "print(list_r2_col)\n",
    "\n",
    "print(\"R2 Score by rows: \" + str(mean_r2_row))\n",
    "print(list_r2_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_metrics` receives as input:\n",
    "\n",
    "* **gt_mat (Union[np.array, torch.Tensor]):** ground truth matrix of shape (n_samples, n_genes).\n",
    "* **pred_mat (Union[np.array, torch.Tensor]):** predicted matrix of shape (n_samples, n_genes).\n",
    "* **mask (Union[np.array, torch.Tensor]):** boolean mask with False in positions that must be ignored in metric computation (n_samples, n_genes).\n",
    "* **detailed (bool):** if True, the dictionary also includes the detailed metrics.\n",
    "\n",
    "And computes the following metrics:\n",
    "    \n",
    "* Pearson correlation (gene-wise) [PCC-Gene]\n",
    "* Pearson correlation (patch-wise) [PCC-Patch]\n",
    "* r2 score (gene-wise) [R2-Gene]\n",
    "* r2 score (patch-wise) [R2-Patch]\n",
    "* Mean squared error [MSE]\n",
    "* Mean absolute error [MAE]\n",
    "* Global metric [Global] (Global = PCC-Gene + R2-Gene + PCC-Patch + R2-Patch - MAE - MSE)\n",
    "    \n",
    "If detailed == True. Then the function returns these aditional keys (all of them are numpy arrays):\n",
    "\n",
    "* Individual pearson correlation for every gene [PPC-Gene-detailed]\n",
    "* Individual pearson correlation for every patch [PPC-Patch-detailed]\n",
    "* Individual r2 score for every gene [R2-Gene-detailed]\n",
    "* Individual r2 score for every patch [R2-Gene-detailed]\n",
    "* Individual MSE for every gene [detailed_mse_gene]\n",
    "* Individual MAE for every gene [detailed_mae_gene]\n",
    "* Individual average error for every gene [detailed_error_gene]\n",
    "* Flat concatenation of all errors in valid positions [detailed_errors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be generating random prediction and ground truth matrices as well as a random mask where *26%* of the values will be masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set number of observations and genes (hypothetical)\n",
    "obs = 10\n",
    "genes = 8\n",
    "imputed_fraction = 0.26 # This is the percentage of zeros in the mask\n",
    "\n",
    "# Henerate random matrices\n",
    "pred = torch.randn((obs,genes))\n",
    "gt = torch.randn((obs,genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics dictionary:\n",
      "{'PCC-Gene': 0.2479938268661499, 'PCC-Patch': 0.21199902892112732, 'R2-Gene': -0.48487168550491333, 'R2-Patch': -2.052999973297119, 'MSE': 1.822934627532959, 'MAE': 1.0708446502685547, 'Global': -4.971658080816269}\n",
      "Detailed metrics dictionary:\n",
      "{'PCC-Gene': 0.2479938268661499, 'PCC-Patch': 0.21199902892112732, 'R2-Gene': -0.48487168550491333, 'R2-Patch': -2.052999973297119, 'MSE': 1.822934627532959, 'MAE': 1.0708446502685547, 'Global': -4.971658080816269, 'detailed_PCC-Gene': [-0.0015925765037536621, 0.22179384529590607, -0.27338194847106934, 0.04636658728122711, 0.773283064365387, 0.6881754398345947, -0.19805178046226501, 0.7273579835891724], 'detailed_PCC-Patch': [0.029849424958229065, 0.5682603120803833, 0.3311068117618561, 0.14430180191993713, 0.1837208867073059, -0.1209012120962143, -0.439031720161438, 0.18234789371490479, 0.9513199925422668, 0.2890160083770752], 'detailed_R2-Gene': [-0.8768579959869385, -0.727558970451355, -1.0784060955047607, -1.4311919212341309, 0.13203251361846924, 0.3312538266181946, -0.6841065883636475, 0.4558621644973755], 'detailed_R2-Patch': [-0.2687453031539917, -0.09128391742706299, -16.569664001464844, -0.5557061433792114, -0.35770976543426514, -1.3734159469604492, -0.751579999923706, -0.08734786510467529, 0.8560383319854736, -1.3305869102478027], 'detailed_mse_gene': [2.510277509689331, 1.3292324542999268, 2.587209463119507, 2.6335878372192383, 1.0729095935821533, 1.3275853395462036, 2.825526237487793, 0.33643245697021484], 'detailed_mae_gene': [1.4261425733566284, 1.008693814277649, 1.1324108839035034, 1.3389551639556885, 0.9124696254730225, 0.9628177285194397, 1.3761475086212158, 0.4588029682636261], 'detailed_error_gene': [-0.9870957732200623, -0.2958986759185791, -0.35661908984184265, 0.2577192485332489, -0.6823060512542725, 0.3390544056892395, 0.25352007150650024, 0.047519609332084656], 'detailed_errors': [0.6729575395584106, 0.26980412006378174, -0.9832386374473572, 0.9206542372703552, -3.045259952545166, -1.0852887630462646, -1.2704386711120605, 0.09988949447870255, -0.06077340245246887, -0.39832502603530884, 0.26726651191711426, 0.8841301798820496, 0.6684855818748474, -1.426755428314209, -1.1350440979003906, 1.7723705768585205, -0.7435076236724854, 0.19048726558685303, -0.7023709416389465, 0.5538265109062195, 2.6738367080688477, -1.7921056747436523, 0.10886380821466446, 0.7606195211410522, -0.6511601209640503, -1.981722354888916, 0.17154842615127563, -0.33733218908309937, 2.2086784839630127, -1.77383553981781, 1.4860190153121948, -0.11432620882987976, -2.408235549926758, -1.8188049793243408, 0.4467872679233551, -0.7374752759933472, 1.2688547372817993, -1.1688990592956543, -0.9740788340568542, -3.7392287254333496, 0.08336138725280762, -0.6847206950187683, 1.6093440055847168, 3.0544824600219727, 0.8555526733398438, 0.6441827416419983, 1.5023863315582275, 1.4081428050994873, -2.075385808944702, -1.2093803882598877, -0.6554703116416931, 0.08136582374572754, 0.9074417352676392, -0.19640469551086426, -0.40933600068092346, 0.030836641788482666, 0.04791635274887085, -1.875678300857544, 0.6534126400947571, -1.7461631298065186, -1.3606905937194824, 1.3022233247756958, 1.9322497844696045, 0.43464386463165283]}\n"
     ]
    }
   ],
   "source": [
    "from spared.metrics import get_metrics\n",
    "\n",
    "dict_metrics = get_metrics(gt_mat = gt, pred_mat = pred, mask = mask, detailed = False)\n",
    "dict_metrics_detailed = get_metrics(gt_mat = gt, pred_mat = pred, mask = mask, detailed = True)\n",
    "\n",
    "print(\"Metrics dictionary:\")\n",
    "print(dict_metrics)\n",
    "print(\"Detailed metrics dictionary:\")\n",
    "print(dict_metrics_detailed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "H2ST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
